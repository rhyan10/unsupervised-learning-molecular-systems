{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6af903c",
   "metadata": {},
   "source": [
    "# Smooth Overlap of Atomic Positions (SOAP) Descriptors\n",
    "Prominant 3D representations for molecules include the [Coulomb matrix](./Coulomb-Matrices.ipynb), Smooth Overlap of Atomic Positions (SOAP), Atom-Centered Symmetry Functions (ACSF), the atomic cluster expansion (ACE), atomic features built by the hierarchically interacting particle neural network (HIP-NN), and the N-body iterative contraction of equivariants (NICE).\n",
    "\n",
    "Here, we demonstrate how to generate SOAP descriptors for molecules.\n",
    "\n",
    "We will use the same kernel as in the [Fingerprints and SMILES](./1-Fingerprints-and-SMILES.ipynb) notebook, and the QM7 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb40ac0a",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "This particular notebook uses `asaplib`, a library containing \"Automatic Selection And Prediction\" tools for materials and molecules. It provides tools for analyzing and visualizing atomic simulation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0dfd70c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load xyz file:  ../data/qm7.xyz , a total of  7165 frames , a total of  110650 atoms , with elements:  [1, 6, 7, 8, 16] .\n"
     ]
    }
   ],
   "source": [
    "from asaplib.data import ASAPXYZ\n",
    "\n",
    "# Import QM7 coordinates using `asaplib`\n",
    "asapxyz = ASAPXYZ('../data/qm7.xyz', periodic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "235dea55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of elements (atomic numbers) found in dataset: [1, 6, 7, 8, 16]\n",
      "Minimal SOAP parameters:\n",
      "{'soap1': {'type': 'SOAP', 'species': [1, 6, 7, 8, 16], 'cutoff': 2.5, 'n': 4, 'l': 3, 'atom_gaussian_width': 0.32}}\n",
      "Updated SOAP parameters:\n",
      "{'soap1': {'type': 'SOAP', 'species': [1, 6, 7, 8, 16], 'cutoff': 2.5, 'n': 4, 'l': 3, 'atom_gaussian_width': 0.32, 'rbf': 'gto', 'crossover': False}}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from asaplib.hypers import universal_soap_hyper\n",
    "\n",
    "# Get the list of elements (atomic numbers) contained in all frames in the data\n",
    "global_species = asapxyz.get_global_species()\n",
    "print(f'List of elements (atomic numbers) found in dataset: {global_species}')\n",
    "\n",
    "# Automatically generate the hyperparameters of SOAP descriptors for the aforementioned\n",
    "# list of elements using the 'minimal' settings and output them (dump=True)\n",
    "universal_soap = 'minimal'\n",
    "print('Minimal SOAP parameters:')\n",
    "soap_spec = universal_soap_hyper(global_species, universal_soap, dump=True)\n",
    "\n",
    "# Modify the hyperparameters as needed\n",
    "for k in soap_spec.keys():\n",
    "    soap_spec[k]['rbf'] = 'gto'       # Radial basis function : Gaussian Type Orbitals\n",
    "    soap_spec[k]['crossover'] = False # Whether or not to include crossover terms between species\n",
    "\n",
    "print('Updated SOAP parameters:')\n",
    "print(soap_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8caa42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify more parameters\n",
    "reducer_spec = {'reducer1': {\n",
    "                          'reducer_type': 'average',  # Options: average, sum, moment_average, moment_sum\n",
    "                          'element_wise': False}\n",
    "               }\n",
    "\n",
    "desc_spec = {'avgsoap': {\n",
    "                  'atomic_descriptor': soap_spec,\n",
    "                  'reducer_function': reducer_spec}\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4703b2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'avgsoap': {'atomic_descriptor': {'soap1': {'type': 'SOAP', 'species': [1, 6, 7, 8, 16], 'cutoff': 2.5, 'n': 4, 'l': 3, 'atom_gaussian_width': 0.32, 'rbf': 'gto', 'crossover': False, 'periodic': False, 'acronym': 'SOAP-n4-l3-c2.5-g0.32'}}, 'reducer_function': {'reducer1': {'reducer_type': 'average', 'element_wise': False, 'species': [1, 6, 7, 8, 16], 'acronym': ''}}, 'species': [1, 6, 7, 8, 16], 'periodic': False, 'max_atoms': 23, 'acronym': 'atomic-to-global-dnjayu'}}\n"
     ]
    }
   ],
   "source": [
    "print(desc_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a77a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SOAP Descriptors ...\n",
      "Using Atomic_2_Global_Average reducer ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'rcut'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/Users/rocom/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/Users/rocom/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/Users/rocom/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/Users/rocom/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/Users/rocom/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/asaplib/descriptors/global_descriptors.py\", line 107, in compute\n    global_desc_dict[element], atomic_desc_dict[element] = self.engines[element].create(frame)\n  File \"/Users/rocom/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/asaplib/descriptors/global_descriptors.py\", line 200, in create\n    atomic_desc_dict = self.atomic_desc.compute(frame)\n  File \"/Users/rocom/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/asaplib/descriptors/atomic_descriptors.py\", line 95, in compute\n    atomic_desc_dict[element]['acronym'], atomic_desc_dict[element]['atomic_descriptors'] = self.engines[\n  File \"/Users/rocom/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/asaplib/descriptors/atomic_descriptors.py\", line 168, in create\n    self.soap = SOAP(species=self.species, rcut=self.cutoff, nmax=self.n, lmax=self.l,\nTypeError: __init__() got an unexpected keyword argument 'rcut'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Compute descriptors for the whole structures\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43masapxyz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_global_descriptors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdesc_spec_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdesc_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43msbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mkeep_atomic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Set to True to keep the atomic descriptors\u001b[39;49;00m\n\u001b[1;32m      5\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mtag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mqm7\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                    \u001b[49m\u001b[43mn_process\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/asaplib/data/xyz.py:288\u001b[0m, in \u001b[0;36mASAPXYZ.compute_global_descriptors\u001b[0;34m(self, desc_spec_dict, sbs, keep_atomic, tag, n_process)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# parallel computation\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m n_process \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 288\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_process\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_desc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msbs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (desc_dict_now, atomic_desc_dict_now) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mglobal_desc[i]\u001b[38;5;241m.\u001b[39mupdate(desc_dict_now)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nb1-env/lib/python3.9/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'rcut'"
     ]
    }
   ],
   "source": [
    "# Compute descriptors for the whole structures\n",
    "asapxyz.compute_global_descriptors(desc_spec_dict=desc_spec,\n",
    "                                    sbs=[],\n",
    "                                    keep_atomic=False,  # Set to True to keep the atomic descriptors\n",
    "                                    tag='qm7',\n",
    "                                    n_process=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5539c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = asapxyz.fetch_computed_descriptors(['avgsoap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b689874",
   "metadata": {},
   "outputs": [],
   "source": [
    "fy = 'atomization_energy'\n",
    "y_train = asapxyz.get_property(fy) #, extensive = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8270ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "\n",
    "# On some computers the explicit cast to .float() is\n",
    "# necessary\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "\n",
    "dataset = TensorDataset(X_train, y_train)\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [0.9, 0.1])\n",
    "\n",
    "batch_size = 1\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecfd9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = nn.Sigmoid()(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 8\n",
    "output_size = 1\n",
    "\n",
    "nnmodel = NNModel(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39148eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nnmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68040e",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.00001\n",
    "\n",
    "loss_fn = nn.MSELoss(reduction='mean')\n",
    "\n",
    "optimizer = torch.optim.Adam(nnmodel.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 100\n",
    "log_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        # 1. Generate predictions\n",
    "        pred = nnmodel(x_batch)[:, 0]\n",
    "\n",
    "        # 2. Calculate loss\n",
    "        loss = loss_fn(pred, y_batch)\n",
    "\n",
    "        # 3. Compute gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # 4. Update parameters using gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        # 5. Reset the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    if epoch % log_epochs==0:\n",
    "        print(f'Epoch {epoch}  Loss {loss.item():.4e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6fb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 5))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "ax.plot(loss_hist, lw=3)\n",
    "ax.set_title('Training loss', size=15)\n",
    "ax.set_xlabel('Epoch', size=15)\n",
    "ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5890e22",
   "metadata": {},
   "source": [
    "## Tips💡\n",
    "\n",
    "### TODO\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
