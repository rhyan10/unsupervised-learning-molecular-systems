{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c89920",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ase'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mase\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ase'"
     ]
    }
   ],
   "source": [
    "import pybel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "import ase.io\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b55b7538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the MolecularVAE class, which inherits from nn.Module\n",
    "class MolecularVAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MolecularVAE, self).__init__()\n",
    "\n",
    "        # Define the layers used in the encoder\n",
    "        self.conv_1 = nn.Conv1d(35, 20, kernel_size=5)\n",
    "        self.conv_2 = nn.Conv1d(20, 15, kernel_size=5)\n",
    "        self.conv_3 = nn.Conv1d(15, 9, kernel_size=5)\n",
    "        self.linear_0 = nn.Linear(90, 200)\n",
    "        self.linear_1 = nn.Linear(200, 200)\n",
    "        self.linear_2 = nn.Linear(200, 200)\n",
    "\n",
    "        # Define the layers used in the decoder\n",
    "        self.linear_3 = nn.Linear(200, 200)\n",
    "        self.gru = nn.GRU(200, 300, 3, batch_first=True)\n",
    "        self.linear_4 = nn.Linear(300, 22)\n",
    "\n",
    "        # Define activation functions\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    # Encoder function\n",
    "    def encode(self, x):\n",
    "        x = self.relu(self.conv_1(x))\n",
    "        x = self.relu(self.conv_2(x))\n",
    "        x = self.relu(self.conv_3(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.selu(self.linear_0(x))\n",
    "        return self.linear_1(x), self.linear_2(x)\n",
    "\n",
    "    # Sampling function for the latent space\n",
    "    def sampling(self, z_mean, z_logvar):\n",
    "        epsilon = 1e-2 * torch.randn_like(z_logvar)\n",
    "        return torch.exp(0.5 * z_logvar) * epsilon + z_mean\n",
    "\n",
    "    # Decoder function\n",
    "    def decode(self, z):\n",
    "        z = F.selu(self.linear_3(z))\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, 35, 1)\n",
    "        output, hn = self.gru(z)\n",
    "        out_reshape = output.contiguous().view(-1, output.size(-1))\n",
    "        y0 = F.softmax(self.linear_4(out_reshape), dim=1)\n",
    "        y = y0.contiguous().view(output.size(0), -1, y0.size(-1))\n",
    "        return y\n",
    "\n",
    "    # Forward pass of the model\n",
    "    def forward(self, x):\n",
    "        z_mean, z_logvar = self.encode(x)\n",
    "        z = self.sampling(z_mean, z_logvar)\n",
    "        return self.decode(z), z_mean, z_logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c2d3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the SMILES strings\n",
    "all_chars = []\n",
    "max_len = 35\n",
    "\n",
    "#Import smiles strings from text files\n",
    "file_path = \"smiles.txt\"\n",
    "smiles_strings = []\n",
    "with open(file_path, \"r\") as file:\n",
    "    smiles_strings = file.readlines()\n",
    "\n",
    "#Create a list of characters in each smiles string then pad with spaces \n",
    "#in order to preprocess for one hot encoding\n",
    "for smi in smiles_strings:\n",
    "    characters = [char for char in smi[:-2]]\n",
    "    while len(characters) < max_len:\n",
    "        characters.append(\" \")\n",
    "    all_chars.append(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57210018",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'smiles_to_hot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m char_indices \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mS\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mo\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m7\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m8\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m9\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m11\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m12\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m13\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m14\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m15\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m16\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m17\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m18\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m19\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m20\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m21\u001b[39m}\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Convert SMILES strings to one-hot encodings\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m one_hot_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43msmiles_to_hot\u001b[49m(all_chars, max_len, char_indices, \u001b[38;5;241m22\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'smiles_to_hot' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the character indices for one-hot encoding\n",
    "char_indices = {'S':0, '2':1, 'O':2, '[':3, 'o':4, '\\\\':5, 'c':6, '3':7, 'C':8, ')':9, 's':10, 'N':11, '(':12, 'H':13, ']':14, '#':15, 'n':16, '1':17, '@':18, '/':19, '=':20, ' ':21}\n",
    "\n",
    "# Convert SMILES strings to one-hot encodings\n",
    "one_hot_embeddings = smiles_to_hot(all_chars, max_len, char_indices, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc8030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test datasets\n",
    "data_train = one_hot_embeddings[:6000]\n",
    "data_test = one_hot_embeddings[6000:]\n",
    "\n",
    "# Create data loaders for train and test datasets\n",
    "data_train = torch.utils.data.TensorDataset(torch.from_numpy(data_train))\n",
    "data_test = torch.utils.data.TensorDataset(torch.from_numpy(data_test))\n",
    "train_loader = torch.utils.data.DataLoader(data_train, batch_size=50, shuffle=True)\n",
    "\n",
    "# Create an instance of the MolecularVAE model\n",
    "model = MolecularVAE()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198184a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "num_epoch = 300\n",
    "for epoch in range(num_epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data[0]\n",
    "        optimizer.zero_grad()\n",
    "        output, mean, logvar = model(data)\n",
    "        loss = vae_loss(output, data, mean, logvar)\n",
    "        loss.backward()\n",
    "        train_loss += loss\n",
    "        optimizer.step()\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    file_path = \"VAEmodel.pth\"\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "    print('Epoch Loss: ' + str(train_loss / len(train_loader.dataset)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
